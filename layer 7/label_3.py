# -*- coding: utf-8 -*-
"""label 3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1maeaxf398WzCa-8RHYOle4vb9R_EToHF
"""

# import libraries
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, mean_squared_error
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.model_selection import cross_val_score, KFold
from sklearn.feature_selection import f_classif
from sklearn.feature_selection import SelectKBest
from sklearn.model_selection import GridSearchCV

# read the test and train data files
train_df = pd.read_csv("train.csv")
valid_df = pd.read_csv("valid.csv")
test_df = pd.read_csv("test.csv")

"""label 03"""

train3_df = train_df.iloc[:,:-1]
valid3_df = valid_df.iloc[:, :-1]
test3_df = test_df.iloc[:, 1:]

train3_df.drop(columns=["label_1", "label_2"], inplace=True)
valid3_df.drop(columns=["label_1", "label_2"], inplace=True)

train3_df.dropna(inplace=True)
valid3_df.dropna(inplace=True)
test3_df.dropna(inplace=True)

# splitting the test and train datasets into X and Y values
X3_train= train3_df.iloc[:,0:-1].values
Y3_train = train3_df.iloc[:,-1].values
X3_valid = valid3_df.iloc[:,0:-1].values
Y3_valid = valid3_df.iloc[:,-1].values
X3_test = test3_df.iloc[:,:].values

# scalling and fitting data
scaler = StandardScaler()
scaler.fit(X3_train)

X3_train = scaler.transform(X3_train)
X3_valid = scaler.transform(X3_valid)
X3_test = scaler.transform(X3_test)

classifiers = [
    ("Random Forest", RandomForestClassifier()),
    ("K-Nearest Neighbors", KNeighborsClassifier(n_neighbors=5)),
    ("Support Vector Machine", SVC(kernel="linear"))
]


# Iterate over each classifier and perform cross-validation
for clf_name, clf in classifiers:
    cross_val_scores = cross_val_score(clf, X3_train, Y3_train, cv=5)

    # Print the cross-validation scores for each classifier
    print(f"{clf_name} Cross-validation scores:", cross_val_scores)

    # Calculate and print the mean and standard deviation of the scores
    print(f"{clf_name} Mean accuracy:", cross_val_scores.mean())
    print(f"{clf_name} Standard deviation:", cross_val_scores.std())
    print("\n")

train3_df['label_3'].value_counts().plot(kind='bar',title='Imbalanced data')

pip install imbalanced-learn

# resampling the data
from imblearn.combine import SMOTETomek

resampler = SMOTETomek(random_state=0)
X3_train, Y3_train = resampler.fit_resample(X3_train, Y3_train)

# Initialize and train a Support Vector Machine classifier
svm_classifier = SVC(kernel='linear', C=1.0)
svm_classifier.fit(X3_train, Y3_train)

# Make predictions on the test set
y_pred = svm_classifier.predict(X3_valid)

print(classification_report(Y3_valid, y_pred))

# Create a SelectKBest instance with a scoring function (e.g., chi-squared)
selector = SelectKBest(score_func=f_classif, k=250)  # Select the top 2 features

# Fit and transform your data to select the best k features
X3_best_train = selector.fit_transform(X3_train, Y3_train)
X3_best_valid = selector.transform(X3_valid)
X3_best_test = selector.transform(X3_test)

svm_classifier.fit(X3_best_train, Y3_train)

# Make predictions on the test set
y_pred = svm_classifier.predict(X3_best_valid)

print(classification_report(Y3_valid, y_pred))

pca=PCA(0.9)
pca = pca.fit(X3_best_train)

x3_train_pca=pca.fit_transform(X3_best_train)
x3_valid_pca = pca.transform(X3_best_valid)
x3_test_pca = pca.transform(X3_best_test)

svm_classifier.fit(x3_train_pca, Y3_train)

# Make predictions on the test set
y_pred = svm_classifier.predict(x3_valid_pca)

print(classification_report(Y3_valid, y_pred))

x3_train_pca.shape

param_grid = {
    'C': [0.1, 1, 10],  # Regularization parameter
    'kernel': ['linear', 'rbf', 'poly'],  # Kernel type
    'gamma': ['scale', 'auto', 0.1, 1]  # Kernel coefficient for 'rbf' and 'poly'
}

# Create a GridSearchCV object with cross-validation
grid_search = GridSearchCV(svm_classifier, param_grid, cv=5, scoring='accuracy', verbose=1, n_jobs=-1)

# Fit the GridSearchCV object to the data to find the best hyperparameters
grid_search.fit(x3_train_pca, Y3_train)

# Print the best hyperparameters and corresponding accuracy
best_params = grid_search.best_params_
best_accuracy = grid_search.best_score_
print("Best Hyperparameters:", best_params)
print("Best Accuracy:", best_accuracy)

best_model = grid_search.best_estimator_
y_pred = best_model.predict(x_1_valid_pca)

print(classification_report(Y1_valid, y_pred))

"""label 02"""

